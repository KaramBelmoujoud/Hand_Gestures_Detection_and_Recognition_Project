{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGI8K15iWN2n"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install dependencies \n",
        "# pip install opencv-python \n",
        "%pip install mediapipe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rodWEAufWN2y"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "N0b9wzQ0WN24"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import uuid\n",
        "import os\n",
        "import copy\n",
        "import itertools\n",
        "from collections import deque, Counter\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y0mtPShWN27"
      },
      "source": [
        "# Initials MP Hands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uKM8UnZWWN28"
      },
      "outputs": [],
      "source": [
        "mp_hands = mp.solutions.hands # hands model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr9LUO97ZRBp"
      },
      "source": [
        "# Detecting Hands Function\n",
        "A list of 21 hand landmarks on the left hand. Each landmark consists of x, y and z. x and y are normalized to [0.0, 1.0] by the image width and height respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pcfh4hlzWN29"
      },
      "outputs": [],
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.flip(image, 1)                     # Mirror display\n",
        "    debug_image = copy.deepcopy(image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
        "    image.flags.writeable = False                  # Image is no longer writeable\n",
        "    results = model.process(image)                 # Make prediction\n",
        "    image.flags.writeable = True                   # Image is now writeable \n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
        "    return image, results ,debug_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6DaZ939iMe1"
      },
      "source": [
        "# Drawing Landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qMdAJSpuWN3A"
      },
      "outputs": [],
      "source": [
        "def draw_landmarks(image, hand):\n",
        "    # Draw Hand connections\n",
        "    mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS, \n",
        "                             mp_drawing.DrawingSpec(color=(33, 225, 225), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(33, 225, 225), thickness=2, circle_radius=2),\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "46FqmyOJ20hA"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_rect(use_brect, image, brect):\n",
        "    if use_brect:\n",
        "        # Outer rectangle\n",
        "        cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
        "                     (0, 0, 0), 1)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Qki7IswT3bfO"
      },
      "outputs": [],
      "source": [
        "def draw_info(image, mode, number):\n",
        "    if 1 <= mode <= 2:\n",
        "        cv2.putText(image, \"Collecting Data \", (10, 90),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1,\n",
        "                   cv2.LINE_AA)\n",
        "        if 0 <= number <= 9:\n",
        "            cv2.putText(image, \"NUM:\" + str(number), (10, 110),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (33, 225, 225), 1,\n",
        "                       cv2.LINE_AA)\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-i2iHLrS4bTW"
      },
      "outputs": [],
      "source": [
        "def draw_info_text(image, brect,finger_gesture_text):\n",
        "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
        "                 (0, 0, 0), -1)\n",
        "\n",
        "    if finger_gesture_text != \"\":\n",
        "        cv2.putText(image, \"Hand Gesture:\" + finger_gesture_text, (10, 60),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
        "        cv2.putText(image, \"Hand Gesture:\" + finger_gesture_text, (10, 60),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2,\n",
        "                   cv2.LINE_AA)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Brw7n4HS9Z9c"
      },
      "outputs": [],
      "source": [
        "def draw_point_history(image, point_history):\n",
        "    for index, point in enumerate(point_history):\n",
        "        if point[0] != 0 and point[1] != 0:\n",
        "            cv2.circle(image, (point[0], point[1]), 1 + int(index / 2),\n",
        "                      (152, 251, 152), 2)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsuViGsDh-oH"
      },
      "source": [
        "# Read labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FDYT9xlThahn"
      },
      "outputs": [],
      "source": [
        "actions = ['goLeft', 'goRight', 'modeDiaPo','modeNormal','write']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LcBv9AvnYUP"
      },
      "source": [
        "# Draw The Rectangle Of Hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KLwP1UM_nYH5"
      },
      "outputs": [],
      "source": [
        "def calc_bounding_rect(image, landmarks):\n",
        "    image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "    landmark_array = np.empty((0, 2), int)\n",
        "\n",
        "    for _, landmark in enumerate(landmarks.landmark):\n",
        "        landmark_x = min(int(landmark.x * image_width), image_width - 1) \n",
        "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
        "\n",
        "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
        "\n",
        "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
        "\n",
        "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
        "\n",
        "    return [x, y, x + w, y + h] # retuen the coords of 4 points of the rect"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q1LOAcnKn-9v"
      },
      "source": [
        "# Extract Keypoint Values of Hand\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ay1ozsC9n-AR"
      },
      "outputs": [],
      "source": [
        "def calc_landmark_list(image, landmarks):\n",
        "    image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "    landmark_point = []\n",
        "\n",
        "    # Keypoint\n",
        "    for _, landmark in enumerate(landmarks.landmark):\n",
        "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
        "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
        "\n",
        "        landmark_point.append([landmark_x, landmark_y])\n",
        "\n",
        "    return landmark_point # keypoint => [[x1,y1],[x2,y2]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmt0kW3hsG40"
      },
      "source": [
        "# Preprocess Data (Normalisation)\n",
        "\n",
        "In our case  Coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HCv-vAPOpbuX"
      },
      "outputs": [],
      "source": [
        "def pre_process_Keypoint_history(image, Keypoints_history):\n",
        "    image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "    temporarly_Keypoints_history = copy.deepcopy(Keypoints_history)\n",
        "\n",
        "    # Convert to relative coordinates\n",
        "    base_x, base_y = 0, 0\n",
        "    for index, point in enumerate(temporarly_Keypoints_history):\n",
        "        if index == 0:\n",
        "            base_x, base_y = point[0], point[1] # point => [x,y]\n",
        "\n",
        "        \n",
        "        # Normalization of Coords\n",
        "        temporarly_Keypoints_history[index][0] = (temporarly_Keypoints_history[index][0] -\n",
        "                                        base_x) / image_width # x / image width\n",
        "        temporarly_Keypoints_history[index][1] = (temporarly_Keypoints_history[index][1] -\n",
        "                                        base_y) / image_height # y / image height\n",
        "\n",
        "    # Convert to a one-dimensional list (Flatten the temporarly_Keypoints_history list to 1d list )\n",
        "    temporarly_Keypoints_history = list(\n",
        "        itertools.chain.from_iterable(temporarly_Keypoints_history))\n",
        "\n",
        "    return temporarly_Keypoints_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "WKojZ8og2Rjg"
      },
      "outputs": [],
      "source": [
        "def pre_process_landmark(landmark_list):\n",
        "    temporarly_landmark_list = copy.deepcopy(landmark_list)\n",
        "\n",
        "    # Convert to relative coordinates\n",
        "    base_x, base_y = 0, 0\n",
        "    for index, landmark_point in enumerate(temporarly_landmark_list):\n",
        "        if index == 0:\n",
        "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
        "\n",
        "        temporarly_landmark_list[index][0] = temporarly_landmark_list[index][0] - base_x\n",
        "        temporarly_landmark_list[index][1] = temporarly_landmark_list[index][1] - base_y\n",
        "\n",
        "    # Convert to a one-dimensional list\n",
        "    temporarly_landmark_list = list(\n",
        "        itertools.chain.from_iterable(temporarly_landmark_list))\n",
        "\n",
        "    # Normalization\n",
        "    max_value = max(list(map(abs, temporarly_landmark_list)))\n",
        "\n",
        "    def normalize_(n):\n",
        "        return n / max_value\n",
        "\n",
        "    temporarly_landmark_list = list(map(normalize_, temporarly_landmark_list))\n",
        "\n",
        "    return temporarly_landmark_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oYe6SGpOtFP7"
      },
      "source": [
        "# Save Collected Coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "do3LhTDks2Vn"
      },
      "outputs": [],
      "source": [
        "def logging_csv(number, mode, point_history_list):\n",
        "    if mode == 0:\n",
        "        pass\n",
        "    if mode == 2 and (0 <= number <= 9):\n",
        "        csv_path = 'Gestures_Presentation_Controle_DataSet/KeyPoints.csv'\n",
        "        with open(csv_path, 'a', newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([number, *point_history_list])\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hghBGX42-hUS"
      },
      "source": [
        "# Choise of Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "RG6j4Uu6-gqo"
      },
      "outputs": [],
      "source": [
        "def select_mode(key, mode):\n",
        "    number = -1\n",
        "    if 48 <= key <= 57:  # 0 ~ 9\n",
        "        number = key - 48\n",
        "    if key == 110:  # n normal\n",
        "        mode = 0\n",
        "    if key == 104:  # h trainig frame\n",
        "        mode = 2\n",
        "    return number, mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "DltqNhrbjSQl"
      },
      "outputs": [],
      "source": [
        "# Parameters Initialisation \n",
        "history_length = 16 # lenght of list that takes max indexes of predections \n",
        "Keypoints_history = deque(maxlen=history_length)\n",
        "Argmax_list = deque(maxlen=history_length)\n",
        "mode = 2 #mode Normal \n",
        "use_boundary_recttangle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "cwSHCLEqf7nJ"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 61\u001b[0m\n\u001b[0;32m     56\u001b[0m        debug_image \u001b[39m=\u001b[39m draw_point_history(debug_image, Keypoints_history)\n\u001b[0;32m     60\u001b[0m        \u001b[39m# Screen reflection \u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m        cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m'\u001b[39;49m\u001b[39mHand Gesture Recognition\u001b[39;49m\u001b[39m'\u001b[39;49m, debug_image)\n\u001b[0;32m     64\u001b[0m cap\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m     65\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n",
            "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
          ]
        }
      ],
      "source": [
        "# Camera preparation\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "\n",
        "# Set mediapipe model \n",
        "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5,max_num_hands=1) as hands: \n",
        "\n",
        "    while cap.isOpened():\n",
        "\n",
        "       # Process Key (ESC: end) \n",
        "       key = cv2.waitKey(10)\n",
        "       if key == 27:  # ESC\n",
        "          break\n",
        "       number, mode = select_mode(key, mode)\n",
        "\n",
        "       # Camera capture #####################################################\n",
        "       ret, frame = cap.read()\n",
        "       if not ret:\n",
        "          break\n",
        "\n",
        "       # Make detections\n",
        "       image, results, debug_image = mediapipe_detection(frame, hands)\n",
        "\n",
        "       if results.multi_hand_landmarks is not None:\n",
        "            for num, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
        "                \n",
        "                # Bounding box calculation\n",
        "                brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
        "\n",
        "                # Landmark calculation\n",
        "                landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
        "\n",
        "                # Conversion to relative coordinates / normalized coordinates\n",
        "                pre_processed_landmark_list = pre_process_landmark(\n",
        "                    landmark_list)\n",
        "                pre_processed_Keypoints_list = pre_process_Keypoint_history(\n",
        "                    debug_image, Keypoints_history)\n",
        "                \n",
        "                # Write to the dataset file\n",
        "                logging_csv(number, mode, pre_processed_Keypoints_list)\n",
        "\n",
        "                # Drawing part\n",
        "                debug_image = draw_bounding_rect(use_boundary_recttangle, debug_image, brect)\n",
        "                debug_image = draw_landmarks(debug_image, hand_landmarks)\n",
        "\n",
        "                debug_image = draw_info_text(\n",
        "                    debug_image,\n",
        "                    brect,\n",
        "                    \"boussaid\",\n",
        "                )\n",
        "\n",
        "       else :\n",
        "         Keypoints_history.append([0, 0])\n",
        "\n",
        "       debug_image = draw_info(debug_image, mode, number)\n",
        "       debug_image = draw_point_history(debug_image, Keypoints_history)\n",
        "\n",
        "\n",
        "\n",
        "       # Screen reflection \n",
        "       cv2.imshow('Hand Gesture Recognition', debug_image)\n",
        "\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBtbMyJ6WN3p"
      },
      "source": [
        "# **Build and Train The Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KrlHFFcC9za"
      },
      "source": [
        "**import necessary package**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gvVQ0uK-_m3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK6bWXlZDNVL"
      },
      "source": [
        "**input shape**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VUm3kZJC8ej"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 16\n",
        "DIMENSION = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkCGElqkDb5N"
      },
      "source": [
        "**Load DataSet and  Features & lables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpSZjKVPD15y"
      },
      "outputs": [],
      "source": [
        "dataset = 'Gestures_Presentation_Controle_DataSet/KeyPoints.csv'\n",
        "model_save_path = 'Gestures_classifier.hdf5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8KrPaiqDbdm"
      },
      "outputs": [],
      "source": [
        "X_features  = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (TIME_STEPS * DIMENSION) + 1)))\n",
        "Y_lables  = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))\n",
        "\n",
        "# split features ana lables into train and test\n",
        "RANDOM_SEED = 42\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, Y_lables, train_size=0.75, random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cl2NdZWFhvz"
      },
      "source": [
        "**Define a Callback to use for EarlyStopping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXEZfStzFfEi"
      },
      "outputs": [],
      "source": [
        "# Model checkpoint callback\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    model_save_path, verbose=1, save_weights_only=False)\n",
        "\n",
        "# callback for earlyStopping\n",
        "Earlystopping_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHrhIdYGEeX0"
      },
      "source": [
        "**Build The Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq12oEAJE37o"
      },
      "outputs": [],
      "source": [
        "# Modl Param\n",
        "use_lstm = False\n",
        "model = None\n",
        "NUM_CLASSES = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKcTUfUBD9Gh"
      },
      "outputs": [],
      "source": [
        "if use_lstm:\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(TIME_STEPS * DIMENSION, )),\n",
        "        tf.keras.layers.Reshape((TIME_STEPS, DIMENSION), input_shape=(TIME_STEPS * DIMENSION, )), \n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.LSTM(16, input_shape=[TIME_STEPS, DIMENSION]),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(10, activation='relu'),\n",
        "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "else:\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(TIME_STEPS * DIMENSION, )),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(10, activation='relu'),\n",
        "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1h-_dbHGYn3"
      },
      "outputs": [],
      "source": [
        "model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPh6RduzGglX"
      },
      "source": [
        "**Train The Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8zinCJ8Gelv"
      },
      "outputs": [],
      "source": [
        "model.fit( X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_test, y_test), callbacks=[cp_callback, Earlystopping_callback] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MYEi2rCHM8W"
      },
      "source": [
        "**Model Summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SdpF4ABHMaf"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ainbBnHaUF"
      },
      "source": [
        "**Save Weights**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TohRVMTjHRRN"
      },
      "outputs": [],
      "source": [
        "model.save(model_save_path, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqgxK0QnH-1F"
      },
      "source": [
        "**Test the model ( Make Predictions )**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVKeVOYEHt29"
      },
      "outputs": [],
      "source": [
        "predict_result = model.predict(np.array([X_test[0]]))\n",
        "print(np.squeeze(predict_result))\n",
        "print(np.argmax(np.squeeze(predict_result)))\n",
        "print(actions[np.argmax(np.squeeze(predict_result))])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST-k7UC4HtIu"
      },
      "source": [
        "Evaluate The Model\n",
        "\n",
        "Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJlhMcP9IrH_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def print_confusion_matrix(y_true, y_pred, report=True):\n",
        "    labels = sorted(list(set(y_true)))\n",
        "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    \n",
        "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
        " \n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
        "    ax.set_ylim(len(set(y_true)), 0)\n",
        "    plt.show()\n",
        "    \n",
        "    if report:\n",
        "        print('Classification Report')\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "Y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print_confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOjMMGaQMTJT"
      },
      "source": [
        "# Convert to model for Tensorflow-Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "918YSOd1MT6w"
      },
      "source": [
        "\n",
        "**Save as a model dedicated to inference**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP65LqTnMYmg"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIvusWueOUeG"
      },
      "outputs": [],
      "source": [
        "tflite_save_path = 'Gestures_classifier.tflite'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_VdeUznPAQH"
      },
      "source": [
        "**Transform model (quantization)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SFZL0OIO5XO"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)  # converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0NFsx5IPLNu"
      },
      "source": [
        "**Inference test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlJdomaNPCdl"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixfMtwVlPNpO"
      },
      "outputs": [],
      "source": [
        "# get input/output tensor\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(input_details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mns-mOWFPUzg"
      },
      "outputs": [],
      "source": [
        "interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MaRZsDCPYQu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Inference implementation\n",
        "interpreter.invoke()\n",
        "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOwMkRnRPmAU"
      },
      "outputs": [],
      "source": [
        "print(np.squeeze(tflite_results))\n",
        "print(np.argmax(np.squeeze(tflite_results)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7a3233f10c65cea83d95747b19164ab9f17459359e8424dc1a176cea08d7ac23"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
