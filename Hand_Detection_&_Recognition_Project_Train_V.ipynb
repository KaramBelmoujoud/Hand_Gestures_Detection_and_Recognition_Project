{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jGI8K15iWN2n"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install dependencies \n",
        "# pip install opencv-python \n",
        "# %pip uninstall mediapipe -v\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rodWEAufWN2y"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N0b9wzQ0WN24"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import copy\n",
        "import itertools\n",
        "from collections import deque, Counter\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y0mtPShWN27"
      },
      "source": [
        "# Initials MP Hands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uKM8UnZWWN28"
      },
      "outputs": [],
      "source": [
        "mp_hands = mp.solutions.hands # hands model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr9LUO97ZRBp"
      },
      "source": [
        "# Detecting Hands Function\n",
        "A list of 21 hand landmarks on the left hand. Each landmark consists of x, y and z. x and y are normalized to [0.0, 1.0] by the image width and height respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pcfh4hlzWN29"
      },
      "outputs": [],
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.flip(image, 1)                     # Mirror display\n",
        "    debug_image = copy.deepcopy(image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
        "    image.flags.writeable = False                  # Image is no longer writeable\n",
        "    results = model.process(image)                 # Make prediction\n",
        "    image.flags.writeable = True                   # Image is now writeable \n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
        "    return image, results ,debug_image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B6DaZ939iMe1"
      },
      "source": [
        "# Drawing Landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qMdAJSpuWN3A"
      },
      "outputs": [],
      "source": [
        "def draw_landmarkss(image, hand):\n",
        "    # Draw Hand connections\n",
        "    mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS, \n",
        "                             mp_drawing.DrawingSpec(color=(33, 225, 225), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(33, 225, 225), thickness=2, circle_radius=2),\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "46FqmyOJ20hA"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_rect(use_brect, image, brect):\n",
        "    if use_brect:\n",
        "        # Outer rectangle\n",
        "        cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
        "                     (0, 0, 0), 1)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qki7IswT3bfO"
      },
      "outputs": [],
      "source": [
        "def draw_info(image, mode, number):\n",
        "    if 1 <= mode <= 2:\n",
        "        cv2.putText(image, \"Collecting Data \", (10, 90),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1,\n",
        "                   cv2.LINE_AA)\n",
        "        if 0 <= number <= 9:\n",
        "            cv2.putText(image, \"NUM:\" + str(number), (10, 110),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (33, 225, 225), 1,\n",
        "                       cv2.LINE_AA)\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-i2iHLrS4bTW"
      },
      "outputs": [],
      "source": [
        "def draw_info_text(image, brect,finger_gesture_text):\n",
        "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
        "                 (0, 0, 0), -1)\n",
        "    \n",
        "    # info_text = handedness.classification[0].label[0:]\n",
        "        \n",
        "    if finger_gesture_text != \"\":\n",
        "        cv2.putText(image, \"Hand Gesture:\" + finger_gesture_text, (10, 60),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
        "        cv2.putText(image, \"Hand Gesture:\" + finger_gesture_text, (10, 60),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2,\n",
        "                   cv2.LINE_AA)\n",
        "        # info_text = info_text + ':' + finger_gesture_text\n",
        "    # cv2.putText(image, info_text, (brect[0] + 5, brect[1] - 4),\n",
        "    #            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Brw7n4HS9Z9c"
      },
      "outputs": [],
      "source": [
        "def draw_point_history(image, point_history):\n",
        "    for index, point in enumerate(point_history):\n",
        "        if point[0]!= 0 and point[1] != 0:\n",
        "            cv2.circle(image, (point[0], point[1]), 1 + int(index / 2),\n",
        "                      (152, 251, 152), 2)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NsuViGsDh-oH"
      },
      "source": [
        "# Read labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FDYT9xlThahn"
      },
      "outputs": [],
      "source": [
        "actions = ['stop','goLeft', 'goRight', 'modeDiaPo','modeNormal']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_LcBv9AvnYUP"
      },
      "source": [
        "# Draw The Rectangle Of Hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KLwP1UM_nYH5"
      },
      "outputs": [],
      "source": [
        "def calc_bounding_rect(image, landmarks):\n",
        "    image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "    landmark_array = np.empty((0, 2), int)\n",
        "\n",
        "    for _, landmark in enumerate(landmarks.landmark):\n",
        "        landmark_x = min(int(landmark.x * image_width), image_width - 1) \n",
        "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
        "\n",
        "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
        "\n",
        "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
        "\n",
        "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
        "\n",
        "    return [x, y, x + w, y + h] # retuen the coords of 4 points of the rect"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q1LOAcnKn-9v"
      },
      "source": [
        "# Extract Keypoint Values of Hand\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ay1ozsC9n-AR"
      },
      "outputs": [],
      "source": [
        "# def calc_landmark_list(image, landmarks):\n",
        "#     image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "#     landmark_point = []\n",
        "\n",
        "#     # Keypoint\n",
        "#     for _, landmark in enumerate(landmarks.landmark):\n",
        "#         landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
        "#         landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
        "\n",
        "#         landmark_point.append([landmark_x, landmark_y])\n",
        "\n",
        "#     return landmark_point # keypoint => [[x1,y1],[x2,y2]]\n",
        "\n",
        "def calc_landmark_list(image, landmarks):\n",
        "    image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "    landmark_point = []\n",
        "\n",
        "    # キーポイント\n",
        "    for _, landmark in enumerate(landmarks.landmark):\n",
        "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
        "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
        "        # landmark_z = landmark.z\n",
        "\n",
        "        landmark_point.append([landmark_x, landmark_y])\n",
        "\n",
        "    return landmark_point"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tmt0kW3hsG40"
      },
      "source": [
        "# Preprocess Data (Normalisation)\n",
        "\n",
        "In our case  Coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HCv-vAPOpbuX"
      },
      "outputs": [],
      "source": [
        "# def pre_process_Keypoint_history(image, Keypoints_history):\n",
        "#     image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "#     temporarly_Keypoints_history = copy.deepcopy(Keypoints_history)\n",
        "\n",
        "#     # Convert to relative coordinates\n",
        "#     base_x, base_y = 0, 0\n",
        "#     for index, point in enumerate(temporarly_Keypoints_history):\n",
        "#         if index == 0:\n",
        "#             base_x, base_y = point[0], point[1]\n",
        "#            # point => [x,y]\n",
        "#          # point => [x,y]\n",
        "\n",
        "        \n",
        "#         # Normalization of Coords\n",
        "#         temporarly_Keypoints_history[index][0] = (temporarly_Keypoints_history[index][0] -\n",
        "#                                         base_x) / image_width # x / image width\n",
        "#         temporarly_Keypoints_history[index][1] = (temporarly_Keypoints_history[index][1] -\n",
        "#                                         base_y) / image_height # y / image height\n",
        "\n",
        "#     # Convert to a one-dimensional list (Flatten the temporarly_Keypoints_history list to 1d list )\n",
        "#     temporarly_Keypoints_history = list(\n",
        "#         itertools.chain.from_iterable(temporarly_Keypoints_history))\n",
        "\n",
        "#     return temporarly_Keypoints_history\n",
        "\n",
        "def pre_process_Keypoint_history(image, point_history):\n",
        "    image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "    temp_point_history = copy.deepcopy(point_history)\n",
        "\n",
        "    base_x, base_y = 0, 0\n",
        "    for index, point in enumerate(temp_point_history):\n",
        "        if index == 0:\n",
        "            base_x, base_y = point[0], point[1]\n",
        "\n",
        "        temp_point_history[index][0] = (temp_point_history[index][0] -\n",
        "                                        base_x) / image_width\n",
        "        temp_point_history[index][1] = (temp_point_history[index][1] -\n",
        "                                        base_y) / image_height\n",
        "\n",
        "    temp_point_history = list(\n",
        "        itertools.chain.from_iterable(temp_point_history))\n",
        "\n",
        "    return temp_point_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WKojZ8og2Rjg"
      },
      "outputs": [],
      "source": [
        "def pre_process_landmark(landmark_list):\n",
        "    temporarly_landmark_list = copy.deepcopy(landmark_list)\n",
        "\n",
        "    # Convert to relative coordinates\n",
        "    base_x, base_y = 0, 0\n",
        "    for index, landmark_point in enumerate(temporarly_landmark_list):\n",
        "        if index == 0:\n",
        "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
        "\n",
        "        temporarly_landmark_list[index][0] = temporarly_landmark_list[index][0] - base_x\n",
        "        temporarly_landmark_list[index][1] = temporarly_landmark_list[index][1] - base_y\n",
        "\n",
        "    # Convert to a one-dimensional list\n",
        "    temporarly_landmark_list = list(\n",
        "        itertools.chain.from_iterable(temporarly_landmark_list))\n",
        "\n",
        "    # Normalization\n",
        "    max_value = max(list(map(abs, temporarly_landmark_list)))\n",
        "\n",
        "    def normalize_(n):\n",
        "        return n / max_value\n",
        "\n",
        "    temporarly_landmark_list = list(map(normalize_, temporarly_landmark_list))\n",
        "\n",
        "    return temporarly_landmark_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oYe6SGpOtFP7"
      },
      "source": [
        "# Save Collected Coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "do3LhTDks2Vn"
      },
      "outputs": [],
      "source": [
        "def logging_csv(number, mode, point_history_list):\n",
        "    if mode == 0:\n",
        "        pass\n",
        "    if mode == 2 and (0 <= number <= 9):\n",
        "        csv_path = 'Gestures_Presentation_Controle_DataSet/Gesture_Keypoints.csv'\n",
        "        with open(csv_path, 'a', newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([number, *point_history_list])\n",
        "    return"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hghBGX42-hUS"
      },
      "source": [
        "# Choise of Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RG6j4Uu6-gqo"
      },
      "outputs": [],
      "source": [
        "def select_mode(key, mode):\n",
        "    number = -1\n",
        "    if 48 <= key <= 57:  # 0 ~ 9\n",
        "        number = key - 48\n",
        "    if key == 110:  # n normal\n",
        "        mode = 0\n",
        "    if key == 104:  # h trainig frame\n",
        "        mode = 2\n",
        "    return number, mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_landmarks(image, landmark_point):\n",
        "    # 接続線\n",
        "    if len(landmark_point) > 0:\n",
        "        # 親指\n",
        "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
        "                (255, 255, 255), 2)\n",
        "\n",
        "        # 人差指\n",
        "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
        "                (255, 255, 255), 2)\n",
        "\n",
        "        # 中指\n",
        "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
        "                (255, 255, 255), 2)\n",
        "\n",
        "        # 薬指\n",
        "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
        "                (255, 255, 255), 2)\n",
        "\n",
        "        # 小指\n",
        "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
        "                (255, 255, 255), 2)\n",
        "\n",
        "        # 手の平\n",
        "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
        "                (255, 255, 255), 2)\n",
        "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
        "                (0, 0, 0), 6)\n",
        "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
        "                (255, 255, 255), 2)\n",
        "\n",
        "    # キーポイント\n",
        "    for index, landmark in enumerate(landmark_point):\n",
        "        if index == 0:  # 手首1\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 1:  # 手首2\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 2:  # 親指：付け根\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 3:  # 親指：第1関節\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 4:  # 親指：指先\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
        "        if index == 5:  # 人差指：付け根\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 6:  # 人差指：第2関節\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 7:  # 人差指：第1関節\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 8:  # 人差指：指先\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
        "        if index == 9:  # 中指：付け根\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 10:  # 中指：第2関節\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 11:  # 中指：第1関節\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 12:  # 中指：指先\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
        "        if index == 13:  # 薬指：付け根\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 14:  # 薬指：第2関節\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 15:  # 薬指：第1関節\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 16:  # 薬指：指先\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
        "        if index == 17:  # 小指：付け根\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 18:  # 小指：第2関節\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 19:  # 小指：第1関節\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
        "        if index == 20:  # 小指：指先\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
        "                      -1)\n",
        "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class PoseClassifier(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path='Model/Sign_classifier_MetaData.tflite',\n",
        "        num_threads=1,\n",
        "    ):\n",
        "        self.interpreter = tf.lite.Interpreter(model_path=model_path,\n",
        "                                               num_threads=num_threads)\n",
        "\n",
        "        self.interpreter.allocate_tensors()\n",
        "        self.input_details = self.interpreter.get_input_details()\n",
        "        self.output_details = self.interpreter.get_output_details()\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        landmark_list,\n",
        "    ):\n",
        "        input_details_tensor_index = self.input_details[0]['index']\n",
        "        self.interpreter.set_tensor(\n",
        "            input_details_tensor_index,\n",
        "            np.array([landmark_list], dtype=np.float32))\n",
        "        self.interpreter.invoke()\n",
        "\n",
        "        output_details_tensor_index = self.output_details[0]['index']\n",
        "\n",
        "        result = self.interpreter.get_tensor(output_details_tensor_index)\n",
        "\n",
        "        result_index = np.argmax(np.squeeze(result))\n",
        "\n",
        "        return result_index\n",
        "\n",
        "\n",
        "Pose_classifier = PoseClassifier()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Classifier(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path='Model/Gestures_classifier_MetaData.tflite',\n",
        "        score_th=0.8,\n",
        "        invalid_value=0,\n",
        "        num_threads=1,\n",
        "    ):\n",
        "        self.interpreter = tf.lite.Interpreter(model_path=model_path,\n",
        "                                               num_threads=num_threads)\n",
        "\n",
        "        self.interpreter.allocate_tensors()\n",
        "        self.input_details = self.interpreter.get_input_details()\n",
        "        self.output_details = self.interpreter.get_output_details()\n",
        "\n",
        "        self.score_th = score_th\n",
        "        self.invalid_value = invalid_value\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        point_history,\n",
        "    ):\n",
        "        input_details_tensor_index = self.input_details[0]['index']\n",
        "        self.interpreter.set_tensor(\n",
        "            input_details_tensor_index,\n",
        "            np.array([point_history], dtype=np.float32))\n",
        "        self.interpreter.invoke()\n",
        "\n",
        "        output_details_tensor_index = self.output_details[0]['index']\n",
        "\n",
        "        result = self.interpreter.get_tensor(output_details_tensor_index)\n",
        "\n",
        "        result_index = np.argmax(np.squeeze(result))\n",
        "\n",
        "        if np.squeeze(result)[result_index] < self.score_th:\n",
        "            result_index = self.invalid_value\n",
        "\n",
        "        return result_index\n",
        "\n",
        "\n",
        "\n",
        "keypoint_classifier = Classifier()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n\\nfrom keras.models import load_model\\nmodel_save_path = 'Model/Gestures_classifier.h5'\\n\\n\\ndef classifier_h5(pre_processed_Keypoints_list):\\n    model = load_model(model_save_path)\\n    predict_result = model.predict(np.expand_dims(pre_processed_Keypoints_list, axis=0))\\n    result_index = np.argmax(np.squeeze(predict_result))\\n    return result_index\\n\\n\\n\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "from keras.models import load_model\n",
        "model_save_path = 'Model/Gestures_classifier.h5'\n",
        "\n",
        "\n",
        "def classifier_h5(pre_processed_Keypoints_list):\n",
        "    model = load_model(model_save_path)\n",
        "    predict_result = model.predict(np.expand_dims(pre_processed_Keypoints_list, axis=0))\n",
        "    result_index = np.argmax(np.squeeze(predict_result))\n",
        "    return result_index\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DltqNhrbjSQl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deque([], maxlen=16)\n"
          ]
        }
      ],
      "source": [
        "# Parameters Initialisation \n",
        "history_length = 16 # lenght of list that takes max indexes of predections \n",
        "Keypoints_history = deque(maxlen=history_length)\n",
        "Argmax_list = deque(maxlen=history_length)\n",
        "mode = 2 #mode Normal \n",
        "use_boundary_recttangle = True\n",
        "print(Argmax_list)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# Camera preparation\\ncap = cv2.VideoCapture(0)\\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 960)\\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 540)\\n\\n# Set mediapipe model\\nwith mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5, max_num_hands=1) as hands:\\n\\n    while cap.isOpened():\\n\\n        # Process Key (ESC: end)\\n        key = cv2.waitKey(10)\\n        if key == 27:  # ESC\\n            break\\n\\n        number, mode = select_mode(key, mode)\\n\\n        # Camera capture #####################################################\\n        ret, frame = cap.read()\\n        if not ret:\\n            break\\n\\n        # Make detections\\n        image, results, debug_image = mediapipe_detection(frame, hands)\\n        ActionDetected = 0\\n        \\n        if results.multi_hand_landmarks:\\n            for hand_landmarks, handedness in zip(results.multi_hand_landmarks,\\n                                                  results.multi_handedness):\\n                \\n                if handedness.classification[0].label == \"Right\" :\\n                    # Bounding box calculation\\n                    brect = calc_bounding_rect(debug_image, hand_landmarks)\\n\\n                    # Landmark calculation\\n                    landmark_list = calc_landmark_list(debug_image, hand_landmarks)\\n\\n                    # Conversion to relative coordinates / normalized coordinates\\n                    pre_processed_landmark_list = pre_process_landmark(\\n                        landmark_list)\\n                    pre_processed_Keypoints_list = pre_process_Keypoint_history(\\n                        debug_image, Keypoints_history)\\n                    \\n                    Keypoints_history.append(landmark_list[8]) \\n\\n                    \\n                    logging_csv(number, mode,\\n                            pre_processed_Keypoints_list)\\n                    \\n                    # Drawing part\\n                    debug_image = draw_bounding_rect(\\n                        use_boundary_recttangle, debug_image, brect)\\n                    debug_image = draw_landmarks(debug_image, landmark_list)\\n                    debug_image = draw_info_text(\\n                        debug_image,\\n                        brect,\\n                        actions[ActionDetected],\\n                    )\\n        else:\\n            Keypoints_history.append([0, 0])\\n\\n        # debug_image = draw_info(debug_image, mode, number)\\n        debug_image = draw_point_history(debug_image, Keypoints_history)\\n\\n        # Screen reflection\\n        cv2.imshow(\\'Hand Gesture Recognition\\', debug_image)\\n\\n\\ncap.release()\\n\\ncv2.destroyAllWindows()\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Camera preparation\n",
        "cap = cv2.VideoCapture(0)\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 960)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 540)\n",
        "\n",
        "# Set mediapipe model\n",
        "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5, max_num_hands=1) as hands:\n",
        "\n",
        "    while cap.isOpened():\n",
        "\n",
        "        # Process Key (ESC: end)\n",
        "        key = cv2.waitKey(10)\n",
        "        if key == 27:  # ESC\n",
        "            break\n",
        "\n",
        "        number, mode = select_mode(key, mode)\n",
        "\n",
        "        # Camera capture #####################################################\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Make detections\n",
        "        image, results, debug_image = mediapipe_detection(frame, hands)\n",
        "        ActionDetected = 0\n",
        "        \n",
        "        if results.multi_hand_landmarks:\n",
        "            for hand_landmarks, handedness in zip(results.multi_hand_landmarks,\n",
        "                                                  results.multi_handedness):\n",
        "                \n",
        "                if handedness.classification[0].label == \"Right\" :\n",
        "                    # Bounding box calculation\n",
        "                    brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
        "\n",
        "                    # Landmark calculation\n",
        "                    landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
        "\n",
        "                    # Conversion to relative coordinates / normalized coordinates\n",
        "                    pre_processed_landmark_list = pre_process_landmark(\n",
        "                        landmark_list)\n",
        "                    pre_processed_Keypoints_list = pre_process_Keypoint_history(\n",
        "                        debug_image, Keypoints_history)\n",
        "                    \n",
        "                    Keypoints_history.append(landmark_list[8]) \n",
        "\n",
        "                    \n",
        "                    logging_csv(number, mode,\n",
        "                            pre_processed_Keypoints_list)\n",
        "                    \n",
        "                    # Drawing part\n",
        "                    debug_image = draw_bounding_rect(\n",
        "                        use_boundary_recttangle, debug_image, brect)\n",
        "                    debug_image = draw_landmarks(debug_image, landmark_list)\n",
        "                    debug_image = draw_info_text(\n",
        "                        debug_image,\n",
        "                        brect,\n",
        "                        actions[ActionDetected],\n",
        "                    )\n",
        "        else:\n",
        "            Keypoints_history.append([0, 0])\n",
        "\n",
        "        # debug_image = draw_info(debug_image, mode, number)\n",
        "        debug_image = draw_point_history(debug_image, Keypoints_history)\n",
        "\n",
        "        # Screen reflection\n",
        "        cv2.imshow('Hand Gesture Recognition', debug_image)\n",
        "\n",
        "\n",
        "cap.release()\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Live Stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "cwSHCLEqf7nJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# Camera preparation\n",
        "cap = cv2.VideoCapture(0)\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 960)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 540)\n",
        "\n",
        "# Set mediapipe model\n",
        "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5, max_num_hands=1) as hands:\n",
        "\n",
        "    while cap.isOpened():\n",
        "\n",
        "        # Process Key (ESC: end)\n",
        "        key = cv2.waitKey(10)\n",
        "        if key == 27:  # ESC\n",
        "            break\n",
        "        # number, mode = select_mode(key, mode)\n",
        "\n",
        "        # Camera capture #####################################################\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Make detections\n",
        "        image, results, debug_image = mediapipe_detection(frame, hands)\n",
        "        ActionDetected = 0\n",
        "        \n",
        "        if results.multi_hand_landmarks:\n",
        "            for hand_landmarks, handedness in zip(results.multi_hand_landmarks,\n",
        "                                                  results.multi_handedness):\n",
        "                \n",
        "                if handedness.classification[0].label == \"Right\" :\n",
        "                    # Bounding box calculation\n",
        "                    brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
        "\n",
        "                    # Landmark calculation\n",
        "                    landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
        "\n",
        "                    # Conversion to relative coordinates / normalized coordinates\n",
        "                    pre_processed_landmark_list = pre_process_landmark(\n",
        "                        landmark_list)\n",
        "                    pre_processed_Keypoints_list = pre_process_Keypoint_history(\n",
        "                        debug_image, Keypoints_history)\n",
        "\n",
        "                    hand_id = Pose_classifier(pre_processed_landmark_list)\n",
        "\n",
        "                    hand_sign_id = 0\n",
        "                    hand_sign_len = len(pre_processed_Keypoints_list)\n",
        "\n",
        "                    if hand_id in (0, 1):\n",
        "                        landmark_index = 8 if hand_id == 0 else 12\n",
        "                        Keypoints_history.append(landmark_list[landmark_index])\n",
        "                    else:\n",
        "                        Keypoints_history.append([0, 0])\n",
        "                        \n",
        "                    if hand_sign_len == (history_length * 2):\n",
        "                        hand_sign_id = keypoint_classifier(\n",
        "                            pre_processed_Keypoints_list)\n",
        "                        \n",
        "                    action_detected = [1, 2] if hand_id == 0 else [3, 4]\n",
        "                    Argmax_list.append(hand_sign_id)\n",
        "                    most_common_fg_id = Counter(\n",
        "                        Argmax_list).most_common()\n",
        "                    if most_common_fg_id[0][0] in action_detected:\n",
        "                        ActionDetected = most_common_fg_id[0][0]\n",
        "                        \n",
        "                    # Drawing part\n",
        "                    debug_image = draw_bounding_rect(\n",
        "                        use_boundary_recttangle, debug_image, brect)\n",
        "                    debug_image = draw_landmarks(debug_image, landmark_list)\n",
        "                    debug_image = draw_info_text(\n",
        "                        debug_image,\n",
        "                        brect,\n",
        "                        actions[ActionDetected],\n",
        "                    )\n",
        "        else:\n",
        "            Keypoints_history.append([0, 0])\n",
        "\n",
        "        # debug_image = draw_info(debug_image, mode, number)\n",
        "        debug_image = draw_point_history(debug_image, Keypoints_history)\n",
        "\n",
        "        # Screen reflection\n",
        "        cv2.imshow('Hand Gesture Recognition', debug_image)\n",
        "\n",
        "\n",
        "cap.release()\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Live Stream v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Camera preparation\n",
        "cap = cv2.VideoCapture(0)\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 960)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 540)\n",
        "\n",
        "# Set mediapipe model\n",
        "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5, max_num_hands=1) as hands:\n",
        "\n",
        "    while cap.isOpened():\n",
        "\n",
        "        # Process Key (ESC: end)\n",
        "        key = cv2.waitKey(10)\n",
        "        if key == 27:  # ESC\n",
        "            break\n",
        "        # number, mode = select_mode(key, mode)\n",
        "\n",
        "        # Camera capture #####################################################\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Make detections\n",
        "        image, results, debug_image = mediapipe_detection(frame, hands)\n",
        "        ActionDetected = 0\n",
        "        \n",
        "        if results.multi_hand_landmarks:\n",
        "            for hand_landmarks, handedness in zip(results.multi_hand_landmarks,\n",
        "                                                  results.multi_handedness):\n",
        "                \n",
        "                if handedness.classification[0].label == \"Right\" :\n",
        "                    # Bounding box calculation\n",
        "                    brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
        "\n",
        "                    # Landmark calculation\n",
        "                    landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
        "\n",
        "                    # Conversion to relative coordinates / normalized coordinates\n",
        "                    pre_processed_landmark_list = pre_process_landmark(\n",
        "                        landmark_list)\n",
        "                    pre_processed_Keypoints_list = pre_process_Keypoint_history(\n",
        "                        debug_image, Keypoints_history)\n",
        "\n",
        "                    hand_id = Pose_classifier(pre_processed_landmark_list)\n",
        "\n",
        "                    hand_sign_id = 0\n",
        "                    hand_sign_len = len(pre_processed_Keypoints_list)\n",
        "\n",
        "                    if hand_id in (0, 1):\n",
        "                        landmark_index = 8 if hand_id == 0 else 12\n",
        "                        Keypoints_history.append(landmark_list[landmark_index])\n",
        "                    else:\n",
        "                        Keypoints_history.append([0, 0])\n",
        "                        \n",
        "                    if hand_sign_len == (history_length * 2):\n",
        "                        hand_sign_id = keypoint_classifier(\n",
        "                            pre_processed_Keypoints_list)\n",
        "                        # print(hand_sign_id)\n",
        "                        \n",
        "                    action_detected = [1, 2] if hand_id == 0 else [3, 4]\n",
        "                    Argmax_list.append(hand_sign_id)\n",
        "                    print(Argmax_list)\n",
        "                    most_common_fg_id = Counter(\n",
        "                        Argmax_list).most_common()\n",
        "                    if most_common_fg_id[0][0] in action_detected:\n",
        "                        ActionDetected = most_common_fg_id[0][0]\n",
        "                        print(actions[ActionDetected])\n",
        "\n",
        "                    # Drawing part\n",
        "                    debug_image = draw_bounding_rect(\n",
        "                        use_boundary_recttangle, debug_image, brect)\n",
        "                    debug_image = draw_landmarks(debug_image, landmark_list)\n",
        "                    debug_image = draw_info_text(\n",
        "                        debug_image,\n",
        "                        brect,\n",
        "                        actions[ActionDetected],\n",
        "                    )\n",
        "        else:\n",
        "            Keypoints_history.append([0, 0])\n",
        "\n",
        "        # debug_image = draw_info(debug_image, mode, number)\n",
        "        debug_image = draw_point_history(debug_image, Keypoints_history)\n",
        "\n",
        "        # Screen reflection\n",
        "        cv2.imshow('Hand Gesture Recognition', debug_image)\n",
        "\n",
        "\n",
        "cap.release()\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7a3233f10c65cea83d95747b19164ab9f17459359e8424dc1a176cea08d7ac23"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
